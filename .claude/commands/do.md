---
name: do
description: Intelligent command router. Analyzes your request and automatically selects the best command, agent, or workflow to execute. Just tell it what you want. Auto-gathers context on first use.
---

# /do Command

**The Universal Command** - Just say what you want, and I'll figure out how to do it.

## Usage

```bash
/do [anything you want]
```

## First Use in Session - Automatic Context Gathering

**On first `/do` of the session**, automatically gather project context:

### Step 0: Context Discovery (First Use Only)

**Execute these searches in parallel**:

1. **Find project CLAUDE.md**:
   ```
   Look for: ./CLAUDE.md, ./.claude/CLAUDE.md, ./docs/CLAUDE.md
   ```

2. **Find key documentation**:
   ```
   Glob: **/*.md (limit to first 20)
   Priority: README.md, CONTRIBUTING.md, ARCHITECTURE.md, docs/*.md
   ```

3. **Detect project type**:
   ```
   Look for: package.json, Cargo.toml, go.mod, requirements.txt, pom.xml
   Extract: project name, version, main dependencies
   ```

4. **Find existing context artifacts**:
   ```
   Look for: ResearchPack*.md, ImplementationPlan*.md, knowledge-core.md
   In: ./, ./.claude/, ./docs/
   ```

5. **Check circuit breaker state**:
   ```
   Read: ~/.claude/.circuit-breaker-state
   ```

**Report context summary**:
```
ğŸ“ Project Context Loaded
   Project: [name from package.json or directory]
   Type: [Node.js/Python/Rust/Go/etc]
   CLAUDE.md: [Found/Not found]
   Existing artifacts: [ResearchPack, Plan, etc. or None]
   Circuit breaker: [CLOSED/OPEN]

Ready to help. What do you need?
```

**Then proceed to intent classification.**

## Examples

```bash
/do add authentication to my API
/do why is this function slow?
/do research how to use Redis
/do check if my context is getting too large
/do deploy to production
/do fix the bug in login.js
/do explain this codebase
```

## How It Works

I analyze your request and automatically route to the best approach:

| Your Intent | I Execute |
|-------------|-----------|
| Build/Add/Create feature | `/workflow` (full automation) |
| Research/Learn/Understand | `/research` + summary |
| Plan/Design/Architect | `/plan` (create blueprint) |
| Implement/Code/Fix (with plan) | `/implement` |
| Debug/Investigate/Why | `@brahma-investigator` |
| Deploy/Release | `@brahma-deployer` |
| Optimize/Performance | `@brahma-optimizer` |
| Monitor/Observe | `@brahma-monitor` |
| Review code/PR/quality | `/review` (multi-perspective) |
| Analyze code/patterns | `@brahma-analyzer` |
| SEO/Search/Rankings | `@seo-strategist` |
| Business analysis/Requirements | `@business-analyst` |
| Content/Marketing/Blog/Social | `@content-strategist` |
| Product strategy/Roadmap/Market | `@product-strategist` |
| Security audit/Vulnerabilities | `@security-auditor` |
| UX review/Accessibility/WCAG | `@ux-accessibility-reviewer` |
| Context/Memory issues | `/context analyze` |
| Complex multi-domain | `@chief-architect` |
| Complex parallel work (3+ layers) | Agent Team (with confirmation) |
| Simple question | Direct answer |

## Decision Logic

**Executing command...**

Analyze the user's request using this decision tree:

### Step 1: Classify Intent

**Keywords indicating WORKFLOW** (full automation):
- "add", "create", "build", "implement", "make", "develop"
- "feature", "functionality", "system", "module"
- "with", "using", "that does"
- Example: "add Redis caching to the API"

**Keywords indicating RESEARCH only**:
- "research", "learn", "understand", "how does", "what is"
- "documentation", "docs", "API reference"
- "best practices", "patterns"
- Example: "research JWT authentication best practices"

**Keywords indicating PLAN only**:
- "plan", "design", "architect", "structure"
- "how should I", "what's the best way"
- "strategy", "approach"
- Example: "plan the database migration strategy"

**Keywords indicating IMPLEMENT only**:
- "implement the plan", "execute", "code this"
- "finish", "complete the implementation"
- Example: "implement the plan we just made"

**Keywords indicating DEBUG/INVESTIGATE**:
- "why", "debug", "investigate", "fix", "broken"
- "error", "bug", "issue", "problem", "failing"
- "not working", "crashed"
- Example: "why is the login failing?"

**Keywords indicating DEPLOY**:
- "deploy", "release", "ship", "push to production"
- "rollout", "launch"
- Example: "deploy v2.0 to production"

**Keywords indicating OPTIMIZE**:
- "optimize", "performance", "slow", "faster"
- "speed up", "improve", "scale"
- Example: "optimize the database queries"

**Keywords indicating MONITOR**:
- "monitor", "observe", "metrics", "logs"
- "alert", "dashboard", "health"
- Example: "set up monitoring for the API"

**Keywords indicating CODE REVIEW**:
- "review", "code review", "PR review", "pull request"
- "review code", "check code quality", "audit code"
- Example: "review the changes in src/auth/"

**Keywords indicating ANALYZE**:
- "analyze", "check", "validate"
- "consistency", "patterns in code"
- Example: "analyze the authentication patterns in codebase"

**Keywords indicating SEO**:
- "seo", "search engine", "rankings", "keywords", "meta tags"
- "organic traffic", "schema markup", "structured data"
- "core web vitals", "google", "search visibility"
- Example: "improve SEO for our landing page"

**Keywords indicating BUSINESS ANALYSIS**:
- "business", "requirements", "stakeholders", "process"
- "roi", "cost-benefit", "kpi", "business logic"
- "swot", "value proposition", "business case"
- Example: "analyze our customer onboarding process"

**Keywords indicating CONTENT/MARKETING**:
- "content", "blog", "social media", "marketing"
- "brand", "messaging", "copywriting", "newsletter"
- "content calendar", "brand voice", "engagement"
- Example: "create content strategy for our product launch"

**Keywords indicating PRODUCT STRATEGY**:
- "product", "roadmap", "market", "competitive"
- "feature prioritization", "go-to-market", "gtm"
- "market analysis", "positioning", "pricing"
- Example: "evaluate market opportunity for new feature"

**Keywords indicating SECURITY**:
- "security", "vulnerability", "audit", "owasp"
- "compliance", "penetration", "breach", "secure"
- "gdpr", "soc2", "hipaa", "encryption"
- Example: "security audit before production deployment"

**Keywords indicating UX/ACCESSIBILITY**:
- "ux", "usability", "accessibility", "wcag"
- "user experience", "a11y", "inclusive design"
- "screen reader", "keyboard navigation", "contrast"
- "heuristic", "user journey", "personas"
- Example: "audit accessibility compliance for our app"

**Keywords indicating CONTEXT issues**:
- "context", "memory", "tokens", "too long"
- "optimize context", "reduce", "clean up"
- Example: "my context is getting too large"

**Keywords indicating COMPLEX orchestration**:
- "complete", "full", "entire", "end-to-end"
- Multiple domains mentioned (frontend + backend + database)
- "everything needed for"
- Example: "build complete e-commerce checkout system"

### Step 1.5: Agent Teams Escalation Check

**BEFORE executing**, evaluate whether the task would benefit from parallel Agent Teams teammates instead of sequential sub-agents.

#### Escalate to Agent Team if ANY of these are true:

**Multi-layer signals** (task touches 3+ distinct layers):
- Frontend + Backend + Database mentioned
- API + UI + Tests mentioned
- "full-stack", "end-to-end", "cross-layer", "complete system"
- Example: "add Stripe payment integration" â†’ DB + backend + frontend + tests

**Explicit parallel request**:
- "en paralelo", "in parallel", "simultaneously", "agent team", "team", "teammates"
- Example: "review this PR with a team"

**Multi-angle audit/review**:
- "full audit", "complete review", "comprehensive audit"
- Security + performance + coverage angles
- Example: "full security audit of the codebase"

**Large refactor signals**:
- "refactor", "migrate", "rewrite" + mentions 5+ files or multiple directories
- "across all modules", "every service", "entire codebase"
- Example: "refactor auth across all microservices"

#### Keep sub-agents (default) if ANY of these are true:
- Task is single-domain (only backend, only frontend, only research)
- User says "quick", "rapido", "simple", "fast"
- Task is research-only, plan-only, or single-file fix
- Task matches a specific agent category (SEO, security, etc.) without multi-layer scope

#### When escalating, propose (never auto-spawn):

Show the user:
```
ğŸš€ Agent Teams Recommendation
   This task touches [X layers]: [list layers detected]
   Recommended team: [N] teammates

   1. [Role 1]: [scope]
   2. [Role 2]: [scope]
   3. [Role 3]: [scope]

   Template: [Code Review Team / Feature Build Team / Debug Team]

   Proceed with Agent Team? Or prefer sequential sub-agents?
```

**If user confirms** â†’ Use the matching template from agent-teams.md to brief teammates
**If user declines** â†’ Continue with normal sub-agent routing (Step 2)

### Step 2: Execute Decision

Based on classification, announce what you're doing:

```
ğŸ¯ Intent detected: [CATEGORY]
ğŸ“‹ Executing: [COMMAND/AGENT]
ğŸ’¡ Reason: [WHY THIS CHOICE]

---
[Execute the chosen command/agent]
```

### Step 3: Fallback Logic

If intent is unclear:
1. Ask ONE clarifying question with options
2. Default to `/workflow` for feature requests
3. Default to `@brahma-investigator` for problems/issues
4. Default to `/research` for questions

### Step 4: Smart Enhancements

**Auto-detect complexity**:
- Simple task (1-2 files) â†’ Direct implementation
- Medium task (3-5 files) â†’ `/workflow`
- Complex task (6+ files, multiple domains) â†’ `@chief-architect`

**Auto-detect urgency**:
- "urgent", "asap", "quick" â†’ Skip extensive research, focus on solution
- "careful", "safe", "production" â†’ Full workflow with all quality gates

**Auto-detect existing context**:
- ResearchPack exists â†’ Skip to `/plan`
- Plan exists â†’ Skip to `/implement`
- Nothing exists â†’ Start with `/research` or `/workflow`

## Examples with Routing

**User**: `/do add user authentication`
```
ğŸ¯ Intent: BUILD FEATURE
ğŸ“‹ Executing: /workflow add user authentication
ğŸ’¡ Reason: "add" + "authentication" = new feature requiring research, planning, and implementation
```

**User**: `/do why is my API slow?`
```
ğŸ¯ Intent: DEBUG/INVESTIGATE
ğŸ“‹ Executing: @brahma-investigator analyze API performance issue
ğŸ’¡ Reason: "why" + "slow" = performance investigation needed
```

**User**: `/do learn about WebSockets`
```
ğŸ¯ Intent: RESEARCH
ğŸ“‹ Executing: /research WebSockets
ğŸ’¡ Reason: "learn about" = research request, no implementation needed
```

**User**: `/do my context is huge`
```
ğŸ¯ Intent: CONTEXT MANAGEMENT
ğŸ“‹ Executing: /context analyze
ğŸ’¡ Reason: "context" + size concern = context optimization needed
```

**User**: `/do deploy the new version`
```
ğŸ¯ Intent: DEPLOYMENT
ğŸ“‹ Executing: @brahma-deployer
ğŸ’¡ Reason: "deploy" = production deployment workflow
```

**User**: `/do improve our SEO rankings`
```
ğŸ¯ Intent: SEO
ğŸ“‹ Executing: @seo-strategist
ğŸ’¡ Reason: "SEO" + "rankings" = search engine optimization audit needed
```

**User**: `/do analyze our customer journey`
```
ğŸ¯ Intent: BUSINESS ANALYSIS
ğŸ“‹ Executing: @business-analyst
ğŸ’¡ Reason: "analyze" + "customer journey" = business process analysis
```

**User**: `/do create blog content for launch`
```
ğŸ¯ Intent: CONTENT/MARKETING
ğŸ“‹ Executing: @content-strategist
ğŸ’¡ Reason: "blog content" + "launch" = content marketing strategy
```

**User**: `/do evaluate market opportunity`
```
ğŸ¯ Intent: PRODUCT STRATEGY
ğŸ“‹ Executing: @product-strategist
ğŸ’¡ Reason: "market opportunity" = market analysis and product strategy
```

**User**: `/do security check before deploy`
```
ğŸ¯ Intent: SECURITY
ğŸ“‹ Executing: @security-auditor
ğŸ’¡ Reason: "security check" + "deploy" = pre-deployment security audit
```

**User**: `/do check accessibility compliance`
```
ğŸ¯ Intent: UX/ACCESSIBILITY
ğŸ“‹ Executing: @ux-accessibility-reviewer
ğŸ’¡ Reason: "accessibility compliance" = WCAG audit and UX review
```

**User**: `/do add Stripe payment integration`
```
ğŸ¯ Intent: BUILD FEATURE (multi-layer)
ğŸš€ Agent Teams Recommendation:
   This task touches 4 layers: DB + backend + frontend + tests
   Recommended: Feature Build Team (3 teammates)
   Proceed with Agent Team? Or prefer sequential sub-agents?
```

**User**: `/do full security audit of the codebase`
```
ğŸ¯ Intent: SECURITY AUDIT (multi-angle)
ğŸš€ Agent Teams Recommendation:
   Multi-angle audit detected: security + performance + test coverage
   Recommended: Code Review Team (3 teammates)
   Proceed with Agent Team? Or prefer sequential sub-agents?
```

**User**: `/do fix the bug in auth middleware`
```
ğŸ¯ Intent: DEBUG/INVESTIGATE
ğŸ“‹ Executing: @brahma-investigator
ğŸ’¡ Reason: Focused single-module task â†’ sub-agents (no escalation)
```

**User**: `/do research Redis caching best practices`
```
ğŸ¯ Intent: RESEARCH
ğŸ“‹ Executing: /research Redis caching
ğŸ’¡ Reason: Research-only â†’ sub-agents (no escalation)
```

## Context Awareness Throughout Session

### Continuous Context Monitoring

Every `/do` command checks:
- **Existing ResearchPack** â†’ Can skip research phase
- **Existing Plan** â†’ Can skip planning phase
- **Recent errors** â†’ May need investigation first
- **Circuit breaker state** â†’ May be blocked
- **Session history** â†’ What we already discussed

### Smart Continuation

If you say `/do continue` or `/do next`:
- Looks at what was done before
- Continues from where we left off
- Example: Research done â†’ Proceeds to Plan

### Context Commands

```bash
/do what's the context?     â†’ Shows current project state
/do clean up context        â†’ Runs /context optimize
/do start fresh             â†’ Runs /context reset
/do what did we do?         â†’ Summarizes session actions
```

## Benefits

- **No need to memorize commands** - Just say what you want
- **Intelligent routing** - I pick the best tool for the job
- **Context aware** - I check what's already done
- **Auto-discovery** - Finds CLAUDE.md, docs, and artifacts
- **Complexity detection** - Simple tasks stay simple
- **Session memory** - Remembers what we've done
- **One entry point** - `/do` handles everything
